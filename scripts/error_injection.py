#!/usr/bin/env python3
"""
Error Injection Module (Tier 2 Augmentation)

–ü—Ä–∏–≤—è–∑–∫–∞ –æ—à–∏–±–æ–∫ –∫ —Å—É–±—Å–∫–æ—Ä–∞–º:
- GRA –Ω–∏–∑–∫–∏–π ‚Üí –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
- LR –Ω–∏–∑–∫–∏–π ‚Üí –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
- FC –Ω–∏–∑–∫–∏–π ‚Üí –ø—Ä–æ–±–ª–µ–º—ã —Å–æ —Å–≤—è–∑–Ω–æ—Å—Ç—å—é
- PR –Ω–∏–∑–∫–∏–π ‚Üí ASR-–∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã (—É–∂–µ —á–∞—Å—Ç–∏—á–Ω–æ –≤ asr_noise_injection.py)
"""

import random
import re
from typing import Tuple

# –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –ø–æ severity
GRAMMAR_ERRORS = {
    "high": [
        # Third person
        ("he go", "he goes"), ("she like", "she likes"), ("it make", "it makes"),
        ("he do", "he does"), ("she have", "she has"), ("it take", "it takes"),
        
        # Articles
        ("I like book", "I like the book"), ("I go to school", "I go to the school"),
        ("I see movie", "I see a movie"), ("I have car", "I have a car"),
        
        # Tenses
        ("I go yesterday", "I went yesterday"), ("I see him tomorrow", "I will see him tomorrow"),
        ("I am go", "I go"), ("I was go", "I went"), ("I will went", "I will go"),
        
        # Word order
        ("I very like", "I like very much"), ("I don't know what do", "I don't know what to do"),
        ("I am agree", "I agree"), ("I am not sure what is", "I am not sure what it is"),
        
        # Prepositions
        ("I go in school", "I go to school"), ("I listen music", "I listen to music"),
        ("I depend from", "I depend on"), ("I interested in", "I am interested in"),
    ],
    "medium": [
        # –ú–µ–Ω–µ–µ —á–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏
        ("he doesn't goes", "he doesn't go"), ("she didn't went", "she didn't go"),
        ("I have been go", "I have been going"), ("I would went", "I would go"),
        ("more better", "better"), ("most good", "best"),
    ],
    "low": [
        # –†–µ–¥–∫–∏–µ, —Ç–æ–Ω–∫–∏–µ –æ—à–∏–±–∫–∏
        ("less people", "fewer people"), ("between you and I", "between you and me"),
    ]
}

# –õ–µ–∫—Å–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ severity
LEXICAL_LIMITATIONS = {
    "high": {
        # –ó–∞–º–µ–Ω—ã –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Å–ª–æ–≤ –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ
        "enjoy": "like",
        "appreciate": "like",
        "significant": "important",
        "crucial": "important",
        "essential": "important",
        "fascinating": "interesting",
        "remarkable": "good",
        "extraordinary": "very good",
        "challenging": "difficult",
        "accomplish": "do",
        "achieve": "do",
        "obtain": "get",
        "acquire": "get",
    },
    "medium": {
        "wonderful": "good",
        "excellent": "good",
        "terrible": "bad",
        "enormous": "big",
        "tiny": "small",
    },
    "low": {
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∑–∞–º–µ–Ω—ã
        "fantastic": "great",
        "incredible": "amazing",
    }
}

# –ü–æ–≤—Ç–æ—Ä —Å–ª–æ–≤ –¥–ª—è –Ω–∏–∑–∫–æ–≥–æ LR
REPETITION_PATTERNS = {
    "high": 0.4,  # 40% —à–∞–Ω—Å –ø–æ–≤—Ç–æ—Ä–∏—Ç—å —Å–ª–æ–≤–æ
    "medium": 0.2,
    "low": 0.1,
}

# –ü—Ä–æ–±–ª–µ–º—ã —Å–æ —Å–≤—è–∑–Ω–æ—Å—Ç—å—é –¥–ª—è –Ω–∏–∑–∫–æ–≥–æ FC
FC_DISFLUENCY = {
    "high": [
        "um...", "uh...", "you know...", "I mean...", "like...",
        "actually, wait...", "let me think...", "what I want to say is...",
    ],
    "medium": [
        "well...", "actually...", "I guess...", "kind of...",
    ],
    "low": [
        "well,", "actually,", "I suppose,",
    ]
}

def get_severity(score: float, threshold_low: float = 4.5, threshold_high: float = 6.5) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç severity –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É–±—Å–∫–æ—Ä–∞"""
    if score <= threshold_low:
        return "high"
    elif score <= threshold_high:
        return "medium"
    else:
        return "low"

def inject_grammar_errors(text: str, gra: float) -> str:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç GRA"""
    severity = get_severity(gra, threshold_low=4.5, threshold_high=6.0)
    
    if severity == "low":
        return text  # –í—ã—Å–æ–∫–∏–π GRA - –º–∏–Ω–∏–º—É–º –æ—à–∏–±–æ–∫
    
    errors = GRAMMAR_ERRORS[severity]
    
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç severity
    if severity == "high":
        max_errors = random.randint(2, 4)
        error_prob = 0.4
    elif severity == "medium":
        max_errors = random.randint(1, 2)
        error_prob = 0.2
    else:
        max_errors = 1
        error_prob = 0.1
    
    error_count = 0
    text_lower = text.lower()
    
    for wrong, correct in errors:
        if error_count >= max_errors:
            break
        
        if correct.lower() in text_lower and random.random() < error_prob:
            # –ó–∞–º–µ–Ω—è–µ–º —Å —É—á–µ—Ç–æ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞
            pattern = re.compile(re.escape(correct), re.IGNORECASE)
            matches = list(pattern.finditer(text))
            if matches:
                match = random.choice(matches)
                # –ó–∞–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
                text = text[:match.start()] + wrong + text[match.end():]
                error_count += 1
                text_lower = text.lower()
    
    return text

def inject_lexical_limits(text: str, lr: float) -> str:
    """–û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –ª–µ–∫—Å–∏–∫—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç LR"""
    severity = get_severity(lr, threshold_low=5.0, threshold_high=6.5)
    
    if severity == "low":
        return text  # –í—ã—Å–æ–∫–∏–π LR - —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–∞—è –ª–µ–∫—Å–∏–∫–∞
    
    limitations = LEXICAL_LIMITATIONS[severity]
    
    # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ
    for advanced, simple in limitations.items():
        if advanced in text.lower():
            # –ó–∞–º–µ–Ω—è–µ–º —Å —É—á–µ—Ç–æ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞
            pattern = re.compile(r'\b' + re.escape(advanced) + r'\b', re.IGNORECASE)
            if random.random() < 0.6:  # 60% —à–∞–Ω—Å –∑–∞–º–µ–Ω–∏—Ç—å
                text = pattern.sub(simple, text)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –¥–ª—è –æ—á–µ–Ω—å –Ω–∏–∑–∫–æ–≥–æ LR
    if severity == "high" and random.random() < REPETITION_PATTERNS["high"]:
        words = text.split()
        if len(words) > 3:
            # –ü–æ–≤—Ç–æ—Ä—è–µ–º –æ–¥–Ω–æ –∏–∑ –ø–µ—Ä–≤—ã—Ö —Å–ª–æ–≤
            word_to_repeat = random.choice(words[:5])
            pos = words.index(word_to_repeat) + 1
            words.insert(pos, f"{word_to_repeat}...")
            text = " ".join(words)
    
    return text

def inject_fc_disfluency(text: str, fc: float) -> str:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å–æ —Å–≤—è–∑–Ω–æ—Å—Ç—å—é –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç FC"""
    severity = get_severity(fc, threshold_low=4.5, threshold_high=6.0)
    
    if severity == "low":
        return text  # –í—ã—Å–æ–∫–∏–π FC - —Ö–æ—Ä–æ—à–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å
    
    disfluencies = FC_DISFLUENCY[severity]
    
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ disfluencies –∑–∞–≤–∏—Å–∏—Ç –æ—Ç severity
    if severity == "high":
        count = random.randint(2, 4)
        prob = 0.5
    elif severity == "medium":
        count = random.randint(1, 2)
        prob = 0.3
    else:
        count = 1
        prob = 0.15
    
    if random.random() < prob:
        words = text.split()
        
        for _ in range(count):
            if len(words) > 2:
                disfluency = random.choice(disfluencies)
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ª—É—á–∞–π–Ω–æ–µ –º–µ—Å—Ç–æ
                pos = random.randint(1, len(words) - 1)
                words.insert(pos, disfluency)
        
        text = " ".join(words)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä—ã–≤—ã –º—ã—Å–ª–µ–π –¥–ª—è –æ—á–µ–Ω—å –Ω–∏–∑–∫–æ–≥–æ FC
    if severity == "high" and random.random() < 0.3:
        # –î–æ–±–∞–≤–ª—è–µ–º "..." –≤ –∫–æ–Ω—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        text = re.sub(r'\.\s+', '... ', text)
        # –ò–Ω–æ–≥–¥–∞ –æ–±—Ä—ã–≤–∞–µ–º –Ω–∞ —Å–µ—Ä–µ–¥–∏–Ω–µ
        if random.random() < 0.2:
            words = text.split()
            if len(words) > 5:
                cut_pos = random.randint(len(words) // 2, len(words) - 2)
                text = " ".join(words[:cut_pos]) + "..."
    
    return text

def inject_errors_by_subscores(text: str, fc: float, lr: float, gra: float, pr: float) -> str:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –¥–æ–±–∞–≤–ª—è–µ—Ç –æ—à–∏–±–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤—Å–µ—Ö —Å—É–±—Å–∫–æ—Ä–æ–≤"""
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ—à–∏–±–∫–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é
    text = inject_grammar_errors(text, gra)
    text = inject_lexical_limits(text, lr)
    text = inject_fc_disfluency(text, fc)
    
    # PR —É–∂–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤ asr_noise_injection.py, –Ω–æ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
    if pr <= 5.0:
        # –î–æ–±–∞–≤–ª—è–µ–º –±–æ–ª—å—à–µ filler words –¥–ª—è –Ω–∏–∑–∫–æ–≥–æ PR
        fillers = ["um", "uh", "er", "erm"]
        if random.random() < 0.3:
            words = text.split()
            if len(words) > 2:
                filler = random.choice(fillers)
                pos = random.randint(1, len(words) - 1)
                words.insert(pos, f"{filler}...")
                text = " ".join(words)
    
    return text

def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ error injection"""
    print("=" * 70)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ERROR INJECTION")
    print("=" * 70)
    
    test_cases = [
        ("I really enjoy reading books in my free time.", 4.0, 4.0, 4.0, 4.5, 5.0),  # –í—Å–µ –Ω–∏–∑–∫–∏–µ
        ("I find this topic quite fascinating and significant.", 6.0, 5.0, 7.0, 6.5, 6.0),  # LR –Ω–∏–∑–∫–∏–π
        ("He goes to school every day and enjoys learning.", 6.0, 7.0, 4.5, 6.5, 6.0),  # GRA –Ω–∏–∑–∫–∏–π
        ("I think this is important. It helps people. It makes life better.", 4.5, 6.0, 6.5, 6.0, 5.5),  # FC –Ω–∏–∑–∫–∏–π
        ("I absolutely appreciate this remarkable opportunity.", 7.0, 7.5, 7.5, 7.0, 7.0),  # –í—Å–µ –≤—ã—Å–æ–∫–∏–µ
    ]
    
    for original, overall, fc, lr, gra, pr in test_cases:
        print(f"\nüìù –û—Ä–∏–≥–∏–Ω–∞–ª: {original}")
        print(f"   Subscores: FC={fc}, LR={lr}, GRA={gra}, PR={pr}")
        
        modified = inject_errors_by_subscores(original, fc, lr, gra, pr)
        print(f"   –° –æ—à–∏–±–∫–∞–º–∏: {modified}")
        
        if original != modified:
            print(f"   ‚úÖ –ò–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω—ã")
        else:
            print(f"   ‚ö†Ô∏è  –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π (–≤–æ–∑–º–æ–∂–Ω–æ, –≤—Å–µ —Å—É–±—Å–∫–æ—Ä—ã –≤—ã—Å–æ–∫–∏–µ)")

if __name__ == '__main__':
    main()

