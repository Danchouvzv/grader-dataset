# üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

## üìä –ß—Ç–æ —É —Ç–µ–±—è –µ—Å—Ç—å

### –î–∞—Ç–∞—Å–µ—Ç v1.3 (–≥–æ—Ç–æ–≤ –∫ –æ–±—É—á–µ–Ω–∏—é)

**–§–∞–π–ª—ã:**
- ‚úÖ `dataset_versions/v1.3/train.csv` - **3376 –æ—Ç–≤–µ—Ç–æ–≤** (–¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
- ‚úÖ `dataset_versions/v1.3/val.csv` - **398 –æ—Ç–≤–µ—Ç–æ–≤** (–¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏)
- ‚úÖ `dataset_versions/v1.3/test.csv` - **490 –æ—Ç–≤–µ—Ç–æ–≤** (–¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞)

**–ß—Ç–æ –≤–Ω—É—Ç—Ä–∏:**
- Part 1: 38.7% / Part 2: 28.8% / Part 3: 32.5%
- Low (‚â§5.5): 39% / Mid (6.0-6.5): 30% / High (‚â•7.0): 31%
- Sample weights: 430 –æ—Ç–≤–µ—Ç–æ–≤ —Å –≤–µ—Å–æ–º 0.4-0.6 (inconsistent)

## üéØ –ß—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å

**–û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞:**
- `target_band_overall` - –æ–±—â–∏–π –±–∞–ª–ª (—Ä–µ–≥—Ä–µ—Å—Å–∏—è, 0.5-band —à–∫–∞–ª–∞)

**–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ (multi-task):**
- `target_band_fc` - Fluency & Coherence
- `target_band_lr` - Lexical Resource
- `target_band_gra` - Grammatical Range & Accuracy
- `target_band_pr` - Pronunciation

## üìù –ö–∞–∫ –æ–±—É—á–∞—Ç—å

### –í–∞—Ä–∏–∞–Ω—Ç 1: Baseline (TF-IDF + LightGBM) - 5 –º–∏–Ω—É—Ç

```python
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.multioutput import MultiOutputRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_absolute_error
import numpy as np

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
train = pd.read_csv('dataset_versions/v1.3/train.csv')
val = pd.read_csv('dataset_versions/v1.3/val.csv')

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
X_train = vectorizer.fit_transform(train['answer_text'])
X_val = vectorizer.transform(val['answer_text'])

# Targets
y_train = train[['target_band_overall', 'target_band_fc', 
                 'target_band_lr', 'target_band_gra', 'target_band_pr']].values
y_val = val[['target_band_overall', 'target_band_fc', 
             'target_band_lr', 'target_band_gra', 'target_band_pr']].values

# Sample weights
weights = train['sample_weight'].astype(float).values

# –û–±—É—á–µ–Ω–∏–µ
model = MultiOutputRegressor(LGBMRegressor(n_estimators=100, random_state=42))
model.fit(X_train, y_train, sample_weight=weights)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
pred = model.predict(X_val)

# –ú–µ—Ç—Ä–∏–∫–∏
mae_overall = mean_absolute_error(y_val[:, 0], pred[:, 0])
print(f"MAE Overall: {mae_overall:.3f}")

# –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ (–æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ –¥–æ 0.5)
pred_rounded = np.round(pred * 2) / 2
accuracy_05 = np.mean(np.abs(pred_rounded[:, 0] - y_val[:, 0]) <= 0.5)
print(f"Accuracy within ¬±0.5: {accuracy_05:.2%}")
```

### –í–∞—Ä–∏–∞–Ω—Ç 2: Encoder (DistilBERT) - 30 –º–∏–Ω—É—Ç –Ω–∞ GPU

```python
from transformers import DistilBERTTokenizer, DistilBERTForSequenceClassification
from transformers import Trainer, TrainingArguments
import torch
from torch.utils.data import Dataset
import pandas as pd

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
train = pd.read_csv('dataset_versions/v1.3/train.csv')
val = pd.read_csv('dataset_versions/v1.3/val.csv')

# Dataset class
class IELTSDataset(Dataset):
    def __init__(self, texts, labels, weights=None):
        self.texts = texts
        self.labels = labels
        self.weights = weights if weights is not None else [1.0] * len(texts)
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        return {
            'text': self.texts[idx],
            'labels': self.labels[idx],
            'weight': self.weights[idx]
        }

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞
tokenizer = DistilBERTTokenizer.from_pretrained('distilbert-base-uncased')
train_texts = train['answer_text'].tolist()
train_labels = train[['target_band_overall']].values
train_weights = train['sample_weight'].astype(float).values

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)

# –û–±—É—á–µ–Ω–∏–µ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä)
# –ü–æ–ª–Ω—ã–π –∫–æ–¥ —Å–º. –≤ docs/TRAINING_GUIDE_V1.3.md
```

## üìà –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏

**–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ:**
- MAE (Mean Absolute Error) –ø–æ `target_band_overall`
- Accuracy within ¬±0.5 band (–ø–æ—Å–ª–µ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏)

**–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ:**
- MAE –ø–æ –∫–∞–∂–¥–æ–º—É subscore (FC, LR, GRA, PR)
- –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —á–∞—Å—Ç—è–º (Part 1, 2, 3 –æ—Ç–¥–µ–ª—å–Ω–æ)
- –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –±—ç–Ω–¥–∞–º (low, mid, high –æ—Ç–¥–µ–ª—å–Ω–æ)

## ‚ö†Ô∏è –í–∞–∂–Ω–æ

1. **–ò—Å–ø–æ–ª—å–∑—É–π sample weights** - 430 –æ—Ç–≤–µ—Ç–æ–≤ –∏–º–µ—é—Ç –≤–µ—Å 0.4-0.6
2. **–ö–∞–ª–∏–±—Ä—É–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è** - –æ–∫—Ä—É–≥–ª—è–π –¥–æ 0.5-band (`round(pred * 2) / 2`)
3. **–¢–µ—Å—Ç–∏—Ä—É–π –Ω–∞ test.csv** —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Ü–µ, –Ω–µ –ø–æ–¥–≥–ª—è–¥—ã–≤–∞–π!

## üìö –ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

- `docs/TRAINING_GUIDE_V1.3.md` - –¥–µ—Ç–∞–ª—å–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ
- `docs/TRAINING_PLAN.md` - –ø–ª–∞–Ω –æ–±—É—á–µ–Ω–∏—è
- `configs/training_config_v1.3.json` - –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

## üéØ –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

**Baseline (TF-IDF + LightGBM):**
- MAE: ~0.4-0.5
- Accuracy ¬±0.5: ~60-70%

**Encoder (DistilBERT):**
- MAE: ~0.3-0.4
- Accuracy ¬±0.5: ~70-80%

**Multi-task (—Å subscores):**
- MAE Overall: ~0.35-0.45
- MAE Subscores: ~0.5-0.6

