# üöÄ –ü–æ–ª–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ Google Colab

**–î–ª—è –Ω–æ–≤–∏—á–∫–æ–≤ - –ø–æ—à–∞–≥–æ–≤–æ, —Å –Ω—É–ª—è**

---

## üìã –ß—Ç–æ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º

1. –ê–∫–∫–∞—É–Ω—Ç Google (–¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ Colab)
2. –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –Ω–∞ GitHub: https://github.com/Danchouvzv/grader-dataset
3. 30-60 –º–∏–Ω—É—Ç –≤—Ä–µ–º–µ–Ω–∏

---

## üéØ –®–∞–≥ 1: –û—Ç–∫—Ä—ã—Ç—å Google Colab

1. –ü–µ—Ä–µ–π–¥–∏ –Ω–∞ https://colab.research.google.com/
2. –ù–∞–∂–º–∏ **"New notebook"** (–∏–ª–∏ **"–§–∞–π–ª" ‚Üí "–ù–æ–≤—ã–π –±–ª–æ–∫–Ω–æ—Ç"**)
3. –ü–µ—Ä–µ–∏–º–µ–Ω—É–π –±–ª–æ–∫–Ω–æ—Ç: –Ω–∞–∂–º–∏ –Ω–∞ "Untitled0" –≤–≤–µ—Ä—Ö—É ‚Üí –≤–≤–µ–¥–∏ "IELTS Model Training"

---

## üì• –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ GitHub

**–°–∫–æ–ø–∏—Ä—É–π –∏ –≤—ã–ø–æ–ª–Ω–∏ —ç—Ç–æ—Ç –∫–æ–¥ –≤ –ø–µ—Ä–≤–æ–π —è—á–µ–π–∫–µ:**

```python
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ git (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
!apt-get update
!apt-get install -y git

# –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
!git clone https://github.com/Danchouvzv/grader-dataset.git

# –ü–µ—Ä–µ—Ö–æ–¥ –≤ –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞
import os
os.chdir('/content/grader-dataset')

# –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ —Ñ–∞–π–ª—ã –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å
!ls -la dataset_versions/v1.1/
```

**–ß—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ:**
- –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Å–∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω –≤ `/content/grader-dataset`
- –î–∞—Ç–∞—Å–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ `dataset_versions/v1.1/`

---

## üì¶ –®–∞–≥ 3: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

**–°–æ–∑–¥–∞–π –Ω–æ–≤—É—é —è—á–µ–π–∫—É –∏ –≤—ã–ø–æ–ª–Ω–∏:**

```python
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
!pip install torch transformers scikit-learn pandas numpy matplotlib tqdm

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏
import torch
print(f"PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
print(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å True –¥–ª—è GPU
```

**–í–∞–∂–Ω–æ:** 
- Colab –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–∞—ë—Ç GPU (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)
- –ï—Å–ª–∏ `CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: True` ‚Üí –±—É–¥–µ—Ç –±—ã—Å—Ç—Ä–æ (20-30 –º–∏–Ω—É—Ç)
- –ï—Å–ª–∏ `False` ‚Üí –±—É–¥–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–æ (2-3 —á–∞—Å–∞), –Ω–æ —Ç–æ–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç

---

## üîß –®–∞–≥ 4: –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è

**–°–æ–∑–¥–∞–π –Ω–æ–≤—É—é —è—á–µ–π–∫—É –∏ —Å–∫–æ–ø–∏—Ä—É–π –≤–µ—Å—å —ç—Ç–æ—Ç –∫–æ–¥:**

```python
# –°–æ–∑–¥–∞—ë–º —Ñ–∞–π–ª train_model.py –ø—Ä—è–º–æ –≤ Colab
%%writefile train_model_colab.py

#!/usr/bin/env python3
"""
–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ IELTS Speaking –≤ Google Colab
"""

import csv
import json
import numpy as np
from collections import defaultdict
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from scipy.stats import spearmanr
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
CONFIG = {
    'model_name': 'distilbert-base-uncased',  # –õ–µ–≥—á–µ –∏ –±—ã—Å—Ç—Ä–µ–µ —á–µ–º BERT
    'max_length': 256,
    'batch_size': 16,
    'learning_rate': 2e-5,
    'epochs': 5,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'random_seed': 42
}

class IELTSDataset(Dataset):
    """Dataset –¥–ª—è IELTS –æ—Ç–≤–µ—Ç–æ–≤"""
    def __init__(self, texts, targets, tokenizer, max_length=256):
        self.texts = texts
        self.targets = targets
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        target = self.targets[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'targets': torch.FloatTensor(target)
        }

class IELTSModel(nn.Module):
    """–ú–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è 5 —Å—É–±—Å–∫–æ—Ä–æ–≤"""
    def __init__(self, model_name, num_outputs=5):
        super(IELTSModel, self).__init__()
        self.encoder = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.3)
        
        # Multi-head –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—É–±—Å–∫–æ—Ä–∞
        self.fc_overall = nn.Linear(self.encoder.config.hidden_size, 1)
        self.fc_fc = nn.Linear(self.encoder.config.hidden_size, 1)
        self.fc_lr = nn.Linear(self.encoder.config.hidden_size, 1)
        self.fc_gra = nn.Linear(self.encoder.config.hidden_size, 1)
        self.fc_pr = nn.Linear(self.encoder.config.hidden_size, 1)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        
        overall = self.fc_overall(pooled_output)
        fc = self.fc_fc(pooled_output)
        lr = self.fc_lr(pooled_output)
        gra = self.fc_gra(pooled_output)
        pr = self.fc_pr(pooled_output)
        
        return torch.cat([overall, fc, lr, gra, pr], dim=1)

def load_data(filepath):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV"""
    texts = []
    targets = []
    user_ids = []
    
    with open(filepath, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            try:
                text = row.get('answer_text', '') or row.get('transcript_raw', '')
                if not text or len(text) < 5:
                    continue
                
                overall = float(row['target_band_overall'])
                fc = float(row['target_band_fc'])
                lr = float(row['target_band_lr'])
                gra = float(row['target_band_gra'])
                pr = float(row['target_band_pr'])
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                if not (3.0 <= overall <= 9.0):
                    continue
                
                texts.append(text)
                targets.append([overall, fc, lr, gra, pr])
                user_ids.append(row.get('user_id', ''))
            except:
                continue
    
    return texts, np.array(targets), user_ids

def split_by_user(texts, targets, user_ids, test_size=0.2):
    """–†–∞–∑–¥–µ–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ user_id"""
    user_to_indices = defaultdict(list)
    for i, uid in enumerate(user_ids):
        user_to_indices[uid].append(i)
    
    users = list(user_to_indices.keys())
    train_users, val_users = train_test_split(users, test_size=test_size, random_state=42)
    
    train_indices = []
    val_indices = []
    
    for uid in train_users:
        train_indices.extend(user_to_indices[uid])
    for uid in val_users:
        val_indices.extend(user_to_indices[uid])
    
    return (np.array(texts)[train_indices], targets[train_indices],
            np.array(texts)[val_indices], targets[val_indices])

def train_epoch(model, dataloader, optimizer, criterion, device):
    """–û–¥–Ω–∞ —ç–ø–æ—Ö–∞ –æ–±—É—á–µ–Ω–∏—è"""
    model.train()
    total_loss = 0
    
    for batch in tqdm(dataloader, desc="Training"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        targets = batch['targets'].to(device)
        
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(dataloader)

def evaluate(model, dataloader, criterion, device):
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            targets = batch['targets'].to(device)
            
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, targets)
            
            total_loss += loss.item()
            all_preds.append(outputs.cpu().numpy())
            all_targets.append(targets.cpu().numpy())
    
    all_preds = np.vstack(all_preds)
    all_targets = np.vstack(all_targets)
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    mae = mean_absolute_error(all_targets, all_preds, multioutput='raw_values')
    mse = mean_squared_error(all_targets, all_preds, multioutput='raw_values')
    rmse = np.sqrt(mse)
    
    # Spearman correlation
    correlations = []
    for i in range(5):
        corr, _ = spearmanr(all_targets[:, i], all_preds[:, i])
        correlations.append(corr if not np.isnan(corr) else 0.0)
    
    return {
        'loss': total_loss / len(dataloader),
        'mae': mae,
        'rmse': rmse,
        'correlations': correlations
    }

def main():
    print("=" * 70)
    print("–û–ë–£–ß–ï–ù–ò–ï IELTS SPEAKING –ú–û–î–ï–õ–ò")
    print("=" * 70)
    
    print(f"\nüîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    print(f"   –ú–æ–¥–µ–ª—å: {CONFIG['model_name']}")
    print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {CONFIG['device']}")
    print(f"   Batch size: {CONFIG['batch_size']}")
    print(f"   Epochs: {CONFIG['epochs']}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    texts, targets, user_ids = load_data('dataset_versions/v1.1/answers.csv')
    print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(texts)} –æ—Ç–≤–µ—Ç–æ–≤")
    
    # Split –ø–æ user_id
    print(f"\nüîÑ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val –ø–æ user_id...")
    train_texts, train_targets, val_texts, val_targets = split_by_user(texts, targets, user_ids)
    print(f"   Train: {len(train_texts)} –æ—Ç–≤–µ—Ç–æ–≤")
    print(f"   Val: {len(val_texts)} –æ—Ç–≤–µ—Ç–æ–≤")
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
    print(f"\nü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {CONFIG['model_name']}...")
    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])
    model = IELTSModel(CONFIG['model_name']).to(CONFIG['device'])
    
    # Datasets –∏ DataLoaders
    train_dataset = IELTSDataset(train_texts, train_targets, tokenizer, CONFIG['max_length'])
    val_dataset = IELTSDataset(val_texts, val_targets, tokenizer, CONFIG['max_length'])
    
    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ loss
    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'])
    criterion = nn.MSELoss()
    
    # –û–±—É—á–µ–Ω–∏–µ
    print(f"\nüöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    best_val_loss = float('inf')
    history = {'train_loss': [], 'val_loss': [], 'val_mae': []}
    
    for epoch in range(CONFIG['epochs']):
        print(f"\nüìä Epoch {epoch + 1}/{CONFIG['epochs']}")
        
        # Train
        train_loss = train_epoch(model, train_loader, optimizer, criterion, CONFIG['device'])
        history['train_loss'].append(train_loss)
        
        # Val
        val_metrics = evaluate(model, val_loader, criterion, CONFIG['device'])
        history['val_loss'].append(val_metrics['loss'])
        history['val_mae'].append(val_metrics['mae'])
        
        print(f"\n   Train Loss: {train_loss:.4f}")
        print(f"   Val Loss: {val_metrics['loss']:.4f}")
        print(f"   Val MAE:")
        print(f"      Overall: {val_metrics['mae'][0]:.3f}")
        print(f"      FC: {val_metrics['mae'][1]:.3f}")
        print(f"      LR: {val_metrics['mae'][2]:.3f}")
        print(f"      GRA: {val_metrics['mae'][3]:.3f}")
        print(f"      PR: {val_metrics['mae'][4]:.3f}")
        print(f"   Val Correlations:")
        print(f"      Overall: {val_metrics['correlations'][0]:.3f}")
        print(f"      FC: {val_metrics['correlations'][1]:.3f}")
        print(f"      LR: {val_metrics['correlations'][2]:.3f}")
        print(f"      GRA: {val_metrics['correlations'][3]:.3f}")
        print(f"      PR: {val_metrics['correlations'][4]:.3f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if val_metrics['loss'] < best_val_loss:
            best_val_loss = val_metrics['loss']
            torch.save(model.state_dict(), 'models/ielts_model_best.pt')
            print(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (loss: {best_val_loss:.4f})")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    print(f"\n" + "=" * 70)
    print("–§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print("=" * 70)
    
    model.load_state_dict(torch.load('models/ielts_model_best.pt'))
    final_metrics = evaluate(model, val_loader, criterion, CONFIG['device'])
    
    print(f"\nüìä –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –Ω–∞ Validation:")
    print(f"   MAE Overall: {final_metrics['mae'][0]:.3f}")
    print(f"   Spearman Overall: {final_metrics['correlations'][0]:.3f}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = {
        'config': CONFIG,
        'final_metrics': {
            'mae': final_metrics['mae'].tolist(),
            'correlations': final_metrics['correlations'],
            'rmse': final_metrics['rmse'].tolist()
        },
        'history': history
    }
    
    import os
    os.makedirs('models', exist_ok=True)
    with open('models/training_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ models/ielts_model_best.pt")
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ models/training_results.json")
    print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

if __name__ == '__main__':
    import os
    os.makedirs('models', exist_ok=True)
    main()
```

**–ß—Ç–æ —ç—Ç–æ –¥–µ–ª–∞–µ—Ç:**
- –°–æ–∑–¥–∞—ë—Ç —Ñ–∞–π–ª `train_model_colab.py` —Å –ø–æ–ª–Ω—ã–º –∫–æ–¥–æ–º –æ–±—É—á–µ–Ω–∏—è
- –í—Å—ë –≥–æ—Ç–æ–≤–æ –∫ –∑–∞–ø—É—Å–∫—É

---

## üöÄ –®–∞–≥ 5: –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ

**–°–æ–∑–¥–∞–π –Ω–æ–≤—É—é —è—á–µ–π–∫—É –∏ –≤—ã–ø–æ–ª–Ω–∏:**

```python
# –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è –º–æ–¥–µ–ª–µ–π
import os
os.makedirs('models', exist_ok=True)

# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
!python train_model_colab.py
```

**–ß—Ç–æ –±—É–¥–µ—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å:**
1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (4022 –æ—Ç–≤–µ—Ç–∞)
2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val (80/20)
3. –ó–∞–≥—Ä—É–∑–∫–∞ DistilBERT –º–æ–¥–µ–ª–∏
4. –û–±—É—á–µ–Ω–∏–µ 5 —ç–ø–æ—Ö
5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏

**–í—Ä–µ–º—è:**
- –° GPU: ~20-30 –º–∏–Ω—É—Ç
- –ë–µ–∑ GPU: ~2-3 —á–∞—Å–∞

---

## üìä –®–∞–≥ 6: –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

**–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω–∏:**

```python
# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
import json
with open('models/training_results.json', 'r') as f:
    results = json.load(f)

print("=" * 70)
print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø")
print("=" * 70)

metrics = results['final_metrics']
print(f"\nüìä MAE (Mean Absolute Error):")
print(f"   Overall: {metrics['mae'][0]:.3f}")
print(f"   FC: {metrics['mae'][1]:.3f}")
print(f"   LR: {metrics['mae'][2]:.3f}")
print(f"   GRA: {metrics['mae'][3]:.3f}")
print(f"   PR: {metrics['mae'][4]:.3f}")

print(f"\nüìà Spearman Correlation:")
print(f"   Overall: {metrics['correlations'][0]:.3f}")
print(f"   FC: {metrics['correlations'][1]:.3f}")
print(f"   LR: {metrics['correlations'][2]:.3f}")
print(f"   GRA: {metrics['correlations'][3]:.3f}")
print(f"   PR: {metrics['correlations'][4]:.3f}")
```

---

## üíæ –®–∞–≥ 7: –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å

**–ß—Ç–æ–±—ã —Å–∫–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä:**

```python
# –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
from google.colab import files

# –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏
files.download('models/ielts_model_best.pt')

# –°–∫–∞—á–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
files.download('models/training_results.json')
```

**–ò–ª–∏ —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å:**
1. –í –ª–µ–≤–æ–π –ø–∞–Ω–µ–ª–∏ –Ω–∞–∂–º–∏ –Ω–∞ –∏–∫–æ–Ω–∫—É –ø–∞–ø–∫–∏ üìÅ
2. –ù–∞–π–¥–∏ `models/ielts_model_best.pt`
3. –ü—Ä–∞–≤—ã–π –∫–ª–∏–∫ ‚Üí "Download"

---

## üéØ –®–∞–≥ 8: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π

**–°–æ–∑–¥–∞–π –Ω–æ–≤—É—é —è—á–µ–π–∫—É –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:**

```python
# –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
import torch
from transformers import AutoTokenizer, AutoModel
import torch.nn as nn

# –ö–ª–∞—Å—Å –º–æ–¥–µ–ª–∏ (—Å–∫–æ–ø–∏—Ä—É–π –∏–∑ train_model_colab.py)
class IELTSModel(nn.Module):
    def __init__(self, model_name):
        super(IELTSModel, self).__init__()
        self.encoder = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.3)
        self.fc_overall = nn.Linear(self.encoder.config.hidden_size, 1)
        self.fc_fc = nn.Linear(self.encoder.config.hidden_size, 1)
        self.fc_lr = nn.Linear(self.encoder.config.hidden_size, 1)
        self.fc_gra = nn.Linear(self.encoder.config.hidden_size, 1)
        self.fc_pr = nn.Linear(self.encoder.config.hidden_size, 1)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        overall = self.fc_overall(pooled_output)
        fc = self.fc_fc(pooled_output)
        lr = self.fc_lr(pooled_output)
        gra = self.fc_gra(pooled_output)
        pr = self.fc_pr(pooled_output)
        return torch.cat([overall, fc, lr, gra, pr], dim=1)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model_name = 'distilbert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = IELTSModel(model_name).to(device)
model.load_state_dict(torch.load('models/ielts_model_best.pt'))
model.eval()

# –ü—Ä–∏–º–µ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
def predict(text):
    encoding = tokenizer(
        text,
        truncation=True,
        padding='max_length',
        max_length=256,
        return_tensors='pt'
    )
    
    with torch.no_grad():
        input_ids = encoding['input_ids'].to(device)
        attention_mask = encoding['attention_mask'].to(device)
        outputs = model(input_ids, attention_mask)
    
    scores = outputs[0].cpu().numpy()
    return {
        'Overall': round(scores[0], 1),
        'FC': round(scores[1], 1),
        'LR': round(scores[2], 1),
        'GRA': round(scores[3], 1),
        'PR': round(scores[4], 1)
    }

# –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ
test_text = "I really enjoy reading books in my free time. I find it quite relaxing and it helps me unwind after a busy day."
result = predict(test_text)

print("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏:")
for criterion, score in result.items():
    print(f"   {criterion}: {score}")
```

---

## ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è

### –ï—Å–ª–∏ Colab –æ—Ç–∫–ª—é—á–∏–ª—Å—è –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:

1. **–°–µ—Å—Å–∏—è Colab –º–æ–∂–µ—Ç –æ—Ç–∫–ª—é—á–∏—Ç—å—Å—è —á–µ—Ä–µ–∑ 90 –º–∏–Ω—É—Ç –Ω–µ–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏**
2. **–†–µ—à–µ–Ω–∏–µ:** –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –¥–≤–∏–≥–∞–π –º—ã—à–∫–æ–π –∏–ª–∏ –¥–æ–±–∞–≤–ª—è–π —è—á–µ–π–∫–∏ —Å `print("still running")`

### –ï—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –ø–∞–º—è—Ç–∏:

```python
# –£–º–µ–Ω—å—à–∏ batch_size –≤ CONFIG
CONFIG['batch_size'] = 8  # –í–º–µ—Å—Ç–æ 16
```

### –ï—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –∏–¥—ë—Ç –º–µ–¥–ª–µ–Ω–Ω–æ:

1. –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ GPU –≤–∫–ª—é—á–µ–Ω: **Runtime ‚Üí Change runtime type ‚Üí GPU**
2. –ï—Å–ª–∏ GPU –Ω–µ—Ç ‚Üí –±—É–¥–µ—Ç –¥–æ–ª–≥–æ, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç

---

## üìù –ß–µ–∫–ª–∏—Å—Ç –¥–ª—è –Ω–æ–≤–∏—á–∫–∞

- [ ] –û—Ç–∫—Ä—ã–ª Google Colab
- [ ] –°–∫–ª–æ–Ω–∏—Ä–æ–≤–∞–ª —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π (–®–∞–≥ 2)
- [ ] –£—Å—Ç–∞–Ω–æ–≤–∏–ª –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (–®–∞–≥ 3)
- [ ] –°–æ–∑–¥–∞–ª train_model_colab.py (–®–∞–≥ 4)
- [ ] –ó–∞–ø—É—Å—Ç–∏–ª –æ–±—É—á–µ–Ω–∏–µ (–®–∞–≥ 5)
- [ ] –î–æ–∂–¥–∞–ª—Å—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è (20-30 –º–∏–Ω—É—Ç)
- [ ] –ü–æ—Å–º–æ—Ç—Ä–µ–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–®–∞–≥ 6)
- [ ] –°–∫–∞—á–∞–ª –º–æ–¥–µ–ª—å (–®–∞–≥ 7)

---

## üÜò –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫

### –û—à–∏–±–∫–∞ "No module named 'transformers'"
‚Üí –í—ã–ø–æ–ª–Ω–∏ —Å–Ω–æ–≤–∞: `!pip install transformers`

### –û—à–∏–±–∫–∞ "CUDA out of memory"
‚Üí –£–º–µ–Ω—å—à–∏ batch_size –¥–æ 8 –∏–ª–∏ 4

### –û—à–∏–±–∫–∞ "File not found"
‚Üí –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ —Ç—ã –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–∞–ø–∫–µ: `!pwd` –¥–æ–ª–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å `/content/grader-dataset`

---

**–ì–æ—Ç–æ–≤–æ! –¢–µ–ø–µ—Ä—å —É —Ç–µ–±—è –µ—Å—Ç—å –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ IELTS Speaking! üéâ**

